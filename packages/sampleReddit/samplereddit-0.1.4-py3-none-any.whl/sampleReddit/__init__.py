# for sampling
import time
import os
import re
import string
import logging
import emojis
import praw
from datetime import datetime
import pandas as pd
from langdetect import detect_langs
from prawcore.exceptions import TooManyRequests


def setup_access(
    client_id: str, client_secret: str, password: str, user_agent: str, username: str
):
    """
    Create an instance for API access.

    Parameters:
        - client_id (str): The client ID for API access.
        - client_secret (str): The client secret for API access.
        - password (str): The password for the Reddit account.
        - user_agent (str): The user agent for API access.
        - username (str): The username for the Reddit account.

    Returns:
        praw.Reddit: An instance of the Reddit API.

    Example:
        >>> instance = setup_access('client_id', 'client_secret', 'password', 'user_agent', 'username')
        >>> print(instance)
        <praw.reddit.Reddit object at 0x7f8a4a7a6f10>

    """

    instance = praw.Reddit(
        client_id=client_id,
        client_secret=client_secret,
        password=password,
        user_agent=user_agent,
        username=username,
    )
    print("API Instance initialized.")
    return instance


def get_posts_list(
    api_instance, subreddit_name: str, post_filter: str, time_period: str, n_posts: int
):
    """
    Get a list of post IDs from a specified subreddit based on the given filter and time period.

    Parameters:
    - api_instance: A PRAW instance (generated by `setup_access`).
    - subreddit_name (str): The name of the subreddit.
    - post_filter (str): The filter type for the posts (top, new, hot).
    - time_period (str): The time period to filter the posts (e.g., 'day', 'week', 'month').
    - n_posts (int): The number of posts to retrieve.

    Returns:
    - list: A list of post IDs.

    """

    # conditions for the filter type
    if post_filter == "top":
        # create the generator
        submission_generator = api_instance.subreddit(subreddit_name).top(
            time_filter=time_period, limit=n_posts
        )
        # return generator outputs as a list
        return [str(submission.id) for submission in submission_generator]

    elif post_filter == "new":
        submission_generator = api_instance.subreddit(subreddit_name).new(
            time_filter=time_period, limit=n_posts
        )
        return [str(submission.id) for submission in submission_generator]

    elif post_filter == "hot":
        submission_generator = api_instance.subreddit(subreddit_name).hot(
            time_filter=time_period, limit=n_posts
        )
        return [str(submission.id) for submission in submission_generator]


def get_post_comments_ids(api_instance, submission_id: str):
    """
    Takes a submission ID and returns a (flattened) list of all its comments.

    Parameters:
    - api_instance: A PRAW instance (generated by `setup_access`).
    - submission_id (str): The ID of the submission.

    Returns:
    - list: A flattened list of all comments on the submission.

    """
    # create a post instance
    comments = api_instance.submission(submission_id).comments
    # replace_more() updates the comment forest by resolving instances of MoreComments
    comments.replace_more()
    # list() flattens the comment forest to a simple list of all comments on the submission
    return [comment.id for comment in comments.list()]


def get_comment_author(api_instance, comment_id: str):
    """
    Get the author of a comment given its ID.

    Parameters:
    - api_instance: A PRAW instance (generated by `setup_access`).
    - comment_id (str): The ID of the comment.

    Returns:
    - The username of the author of the comment.

    """
    return str(api_instance.comment(comment_id).author)


def sample_reddit(
    api_instance,
    seed_subreddits: list,
    post_filter: str,
    time_period: str,
    n_posts: int,
    log_file_path: str,
):
    """
    Generate a snowball sample from a list of subreddits. From the subreddits, get a
    list `n_submissions` number of of posts from `time_period`. From each post, get a list of
    comments and from each comment, get the author. Write the author to a CSV and return a
    dictionary that maps the subreddit to the posts, the posts to the comments, and the comments
    to the authors.

    Parameters:
    - api_instance: A PRAW instance (generated by `setup_access`).
    - seed_subreddits (list): A list of seed subreddits.
    - post_filter (str): The filter type for the posts (top, new, hot).
    - time_period (str): The time period to filter the posts (e.g., 'day', 'week', 'month').
    - n_posts (int): The number of posts to retrieve.
    - log_file_path (str): The file path for the log file.

    Returns:
    - tuple: A tuple containing the output dictionary and a list of users.

    Example:
        >>> instance = setup_access('client_id', 'client_secret', 'password', 'user_agent', 'username')
        >>> seed_subreddits = ['subreddit1', 'subreddit2']
        >>> post_filter = 'top'
        >>> time_period = 'day'
        >>> n_posts = 10
        >>> log_file_path = 'log.txt'
        >>> output, users = sample_reddit(instance, seed_subreddits, post_filter, time_period, n_posts, log_file_path)
        >>> print(output)
        {
            'subreddits_to_posts': {'subreddit1': ['post1', 'post2'], 'subreddit2': ['post3', 'post4']},
            'posts_to_comments': {'post1': ['comment1', 'comment2'], 'post2': ['comment3', 'comment4']},
            'comments_to_users': {'comment1': 'user1', 'comment2': 'user2'},
            'users': ['user1', 'user2']
        }
        >>> print(users)
        ['user1', 'user2']

    """
    start_time = time.time()

    # initialize sample logging dicts
    subreddits_to_posts = {}
    posts_to_comments = {}
    comments_to_users = {}
    users = {"users": []}

    # iterate through the seed subreddits, getting a list of top posts IDs
    for seed in seed_subreddits:

        posts = get_posts_list(
            api_instance=api_instance,
            subreddit_name=seed,
            post_filter=post_filter,
            time_period=time_period,
            n_posts=n_posts,
        )

        # add a key "seed" with the post IDs as items
        subreddits_to_posts.update({seed: posts})

        # iterate through the posts, retreiving their comment IDs
        for i, post in enumerate(posts):
            comments = get_post_comments_ids(
                api_instance=api_instance, submission_id=post
            )
            posts_to_comments.update({post: comments})

            # iterate through the comments, retreiving each one's author
            for comment in comments:
                user = get_comment_author(api_instance=api_instance, comment_id=comment)
                # add the new comment/user pair to the dict
                comments_to_users.update({comment: user})
                # append user/comment pair to dict
                users["users"].append(user)

                time.sleep(0.5)
            # logging
            log_string = f'{datetime.now()} - Finished Post {i + 1} of seed "{seed}"\n'
            log_to_file(f"{log_file_path}sample_log{datetime.now()}.txt", log_string)

    output_dict = {
        "subreddits_to_posts": subreddits_to_posts,
        "posts_to_comments": posts_to_comments,
        "comments_to_users": comments_to_users,
        "users": users["users"],
    }

    print(f"Sample complete. Time elapsed: {(time.time() - start_time) / 60} minutes")

    return output_dict, users


def get_user_comments(
    api_instance,
    user_id: str,
    out_file_path: str,
    log_path: str,
    n_retries=3,
    limit=1000,
):
    """
    Takes a user ID and collects "limit" number (up to 1,000) of that
    user's most recent comments, with metadata. Filters "distinguished"
    comments, which are used to add a "MOD" decorator (used when engaging as a
    moderator rather than a community member). Writes data to disk one row at
    a time as a CSV.

    Args:
        - api_instance: A PRAW instance (generated by `setup_access`).
        - user_id (str): The ID of the user whose comments will be retrieved.
        - out_file_path (str): The file path where the comments will be saved.
        - log_path (str): The file path where the log will be saved.
        - n_retries (int, optional): The number of retries in case of API errors. Defaults to 3.
        - limit (int, optional): The maximum number of comments to retrieve. Maximum is 1000. Defaults to 1000.

    Returns:
        None

    Side Effects:
        Writes comment data to a CSV file per iteration.

    Raises:
        ValueError: If the user ID is invalid.
        IOError: If there is an error saving the comments to the file.
        APIError: If there is an error retrieving the comments from the API.
    """

    # get a ListingGenerator for up to the user's 1,000 most recent comments
    user_comment_generator = api_instance.redditor(user_id)

    col_names = [
        "comment_id",
        "username",
        "post_id",
        "subreddit_id",
        "timestamp",
        "parent_comment",  # if top-level, then returns the submission ID
        "upvotes",
        "text",
    ]

    # retry loop
    for i in range(n_retries):
        try:
            # iterate over the generator to call each comment by the user
            for comment in user_comment_generator.comments.new(limit=limit):
                # don't collect distinguished comments
                if comment.distinguished != "moderator":
                    # data to collect
                    comment_metadata = [
                        comment.id,
                        user_id,
                        comment.link_id,
                        comment.subreddit_id,
                        comment.created_utc,
                        comment.parent_id,
                        comment.score,
                        comment.body,
                    ]

                    data_row = pd.DataFrame([comment_metadata], columns=col_names)
                    # check if the file exists
                    file_exists = True if os.path.exists(out_file_path) else False
                    if file_exists is False:
                        with open(out_file_path, "w") as file:
                            data_row.to_csv(file, index=False, header=True)
                    else:
                        with open(out_file_path, "a") as file:
                            data_row.to_csv(file, index=False, header=False)
                # get another comment to account for skipping the mod comment
            # exit retry loop
            break

        # if a TooManyRequsts error is raised then the API rate limit has been exceeded.
        # Retry after sleeping. Sleep duration increases by a factor of 2 for 3 retries.
        except TooManyRequests as e:
            log_to_file(
                log_path, f"Error: {e} while fetching one of {user_id}'s comments\n"
            )
            print(f"Error: {e} while fetching user {user_id}")
            sleep_time = 1 * (2**i)  # each retry waits for longer: 1s, 2s, 4s
            print(f"Making {i + 1}st retry after waiting {sleep_time}s")
            time.sleep(sleep_time)

        # catch all other possible exceptions and break retry loop
        except Exception as e:
            log_to_file(
                log_path,
                f'Unresolved Error: "{e}" while fetching one of {user_id}\'s comments\n',
            )
            print(f'Error: "{e}" while fetching user {user_id}')
            break


def get_comments(
    api_instance,
    input_path: str,
    output_path: str,
    log_path: str,
    comment_limit=1000,
):
    """
    Fetches comments from Reddit for a list of users and saves the comment metadata to a CSV file.
    Uses the `get_user_comments` function to retrieve the comments for each user.

    Parameters:
        - api_instance: A PRAW instance (generated by `setup_access`).
        - input_path (str): The file path of the input CSV file containing the list of users.
        - output_path (str): The file path where the comment metadata will be saved.
        - log_path (str): The file path where the log will be saved.
        - comment_limit (int, optional): The maximum number of comments to retrieve per user. Maximum is 1000. Defaults to 1000.

    Returns:
        None

    Side Effects:
        - Writes comment metadata to a CSV file for each user.
        - Logs progress and time estimates to a log file.

    Raises:
        - ValueError: If the input CSV file is invalid or empty.
        - IOError: If there is an error saving the comment metadata to the file.
        - APIError: If there is an error retrieving the comments from the API.

    Example:
        >>> api_instance = setup_access('client_id', 'client_secret', 'password', 'user_agent', 'username')
        >>> get_comments(api_instance, 'input.csv', 'output.csv', 'log.txt', comment_limit=500)

    """
    # initialize log file
    start_time = time.time()
    log_to_file(log_path, f"{datetime.now()} - Begin Fetching comments...\n")
    # setup a PRAW reddit instance
    # read in users subset
    users = pd.read_csv(input_path)
    users_list = list(users["users"])
    # iterate over list of user, extracting each user's comment metadata
    for i, user in enumerate(users_list):
        # initialize dict to store a single user's comments
        # make comment metadata dict using reddit API
        get_user_comments(
            api_instance=api_instance,
            user_id=user,
            limit=comment_limit,
            out_file_path=output_path,
            log_path=log_path,
        )
        estimate = estimate_time_remaining(
            task_index=i, total_tasks=len(users_list), start_time=start_time
        )
        print(f"Finished collecting comment data for user {i + 1}")
        print(f"Time remaining: ~{estimate} hours")
    # final logging
    total_time_hours = (time.time() - start_time) / 3600
    print(f"Total Time Elapsed: {total_time_hours}")
    log_to_file(log_path, f"Total Time Elapsed: {total_time_hours}")


def get_user_metadata(
    api_instance,
    user_id: str,
    out_file_path: str,
    log_path: str,
    n_retries=3,
):
    """
    Get user metadata from the Reddit API.

    Parameters:
        - api_instance: A PRAW instance (generated by `setup_access`).
        - user_id (str): The ID of the user to fetch metadata for.
        - out_file_path (str): The path to the output CSV file.
        - log_path (str): The path to the log file.
        - n_retries (int, optional): The number of retries in case of errors. Defaults to 3.

    Returns:
        None

    Side Effects:
        - Appends comment metadata to a CSV file for each user.
        - Logs progress and time estimates to a log file.

    Raises:
        TooManyRequests: If the API rate limit has been exceeded.

    Notes:
        - This function collects metadata for a given user from the Reddit API.
        - The metadata includes the user's display name, ID, comment karma, total karma, and creation timestamp.
        - The metadata is saved to a CSV file specified by the 'out_file_path' parameter.
        - In case of errors, the function retries a specified number of times and logs the errors to a file specified by the 'log_path' parameter.
        - If the API rate limit is exceeded, a 'TooManyRequests' exception is raised.

    Example:
        >>> get_user_metadata(api, "example_user", "output.csv", "log.txt", n_retries=5)
    """
    # column names for csv output
    col_names = ["display_name", "id", "comment_karma", "total_karma", "created_utc"]
    # get a ListingGenerator for up to the user's 1,000 most recent comments
    user = api_instance.redditor(user_id)

    # retry loop
    for i in range(n_retries):
        try:
            # iterate over the generator to call each comment by the user
            # get another comment to account for skipping the mod commen
            # data to collect
            metadata_list = [
                user_id,
                user.id,
                user.comment_karma,
                user.total_karma,
                user.created_utc,
            ]

            print(f'Finished collecting metadata for user "{user}"')
            # stream data to CSV file
            data_row = pd.DataFrame([metadata_list], columns=col_names)
            # check if the file exists
            file_exists = True if os.path.exists(out_file_path) else False
            if file_exists is False:
                with open(out_file_path, "w") as file:
                    data_row.to_csv(file, index=False, header=True)
            else:
                with open(out_file_path, "a") as file:
                    data_row.to_csv(file, index=False, header=False)
            # exit retry loop
            break

        # if a TooManyRequsts error is raised then the API rate limit has been exceeded.
        # Retry after sleeping. Sleep duration increases by a factor of 2 for 4 retries.
        except TooManyRequests as e:
            log_to_file(
                log_path, f'Error: {e} while fetching metadata for "{user_id}\n"'
            )
            print(f'Error: {e} while fetching metadata for "{user_id}"')
            sleep_time = 1 * (2**i)  # each retry waits for longer: 1s, 2s, 4s
            print(f"Making {i + 1}st retry after waiting {sleep_time}s")
            time.sleep(sleep_time)

        # catch all other possible exceptions and break retry loop
        except Exception as e:
            log_to_file(
                log_path,
                f'Unresolved Error: "{e}" while fetching "{user_id}"\'s metadata\n',
            )
            print(f'Error: "{e}" while fetching user {user_id}')
            break


"""Utilities for logging and data processing."""


def process_user_ids(id_list: str):
    """
    Process a list of user IDs and remove duplicates and unwanted values.

    Parameters:
    - id_list (list): A list of user IDs.

    Returns:
    - list: A list of user IDs with duplicates and unwanted values removed.

    Example:
    >>> process_user_ids(['user1', 'user2', 'user1', 'None', 'user3', 'AutoModerator'])
    ['user1', 'user2', 'user3']
    """
    no_dupes = list(set(id_list))

    return [user for user in no_dupes if user not in ("None", "AutoModerator")]


def log_to_file(path: str, message: str):
    """
    Write a message to a log file.

    Parameters:
    - path (str): The file path of the log file.
    - message (str): The message to be written to the log file.

    Returns:
    None

    Side Effects:
    - Appends the message to the log file specified by the 'path' parameter.

    Example:
        >>> log_to_file('log.txt', 'This is a log message.')

    """
    with open(f"{path}", "a") as file:
        file.write(message)


def estimate_time_remaining(task_index: int, total_tasks: int, start_time: float):
    """
    Estimates the remaining time for a given task based on the elapsed time, task index, and total number of tasks.

    Parameters:
        - task_index (int): The index of the current task.
        - total_tasks (int): The total number of tasks.
        - start_time (float): The start time of the task in seconds.

    Returns:
        float: The estimated remaining time in hours.

    Example:
        >>> estimate_time_remaining(2, 10, 1609459200)
        1.8

    """
    elapsed = (time.time() - start_time) / 3600  # convert seconds to hours
    t_per_task = elapsed / (task_index + 1)
    estimate = t_per_task * (total_tasks - task_index)

    return estimate


def remove_urls(comment: str):
    """
    Coerce to string and remove URLs.

    Parameters:
        comment (str): The input comment string from which URLs will be removed.

    Returns:
        str: The cleaned comment string with URLs removed.

    Raises:
        None

    Example:
        >>> comment = "Check out this website: https://www.example.com"
        >>> remove_urls(comment)
        'Check out this website: '
    """

    pattern = re.compile(
        r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
    )
    try:
        clean = re.sub(pattern, "", comment)
    except Exception as e:
        print(f"An error occurred during URL removal: {e}")
        clean = comment

    return clean


def remove_emojis(comment: str):
    """
    Remove emojis from a string.

    Parameters:
        comment (str): The input string from which emojis will be removed.

    Returns:
        str: The input string with emojis removed.

    Example:
        >>> remove_emojis("I love pizza ðŸ•")
        'I love pizza'
    """
    emoji_pattern = re.compile(r":\w+:")
    decoded = emojis.decode(comment)
    clean_word_list = [re.sub(emoji_pattern, "", word) for word in decoded.split()]

    return " ".join(clean_word_list)


def check_language(comment: str):
    """
    Check the language of a given comment.

    Parameters:
    - comment (str): The comment to be checked.

    Returns:
    - str or None: If the comment is in English, return the comment. Otherwise, return None.

    Notes:
    - This function removes punctuation from the comment.
    - If the comment is empty, None is returned.
    - Single word comments are assumed to be in English.
    - To speed up processing, only the first 20 words of the comment are used for language classification.
    - The language of the comment is determined using the detect_langs function from the langdetect library.
    - If an error occurs during language detection, None is returned and the error is logged.

    """
    # Catch some basic problematic cases

    # remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    _comment = comment.translate(translator)

    # store the result of `str(comment).split()` in a variable
    comment_words = str(_comment).split()

    # if the comment is empty, return None
    if len(comment_words) == 0:
        return None

    # assume that single word comments are in English
    # if its one word then langdetect often misclassified.
    if len(comment_words) == 1:
        return comment

    # speed things up by only classifying based on the first 20 words
    if len(comment_words) > 20:
        _comment = " ".join(comment_words[:20])

    # check the language
    # if detect_langs throws an error, return None
    try:
        langs_raw = detect_langs(_comment)

        lang_prob = str(langs_raw[0]).split(":")
        langs = lang_prob[0]
        probs = lang_prob[1]
        langs_dict = {langs: float(probs)}

        highest_prob = max(langs_dict.values())

        if langs_dict.get("en") == highest_prob:
            return comment
        else:
            return None

    except Exception as e:
        logging.error(e)
        return None
