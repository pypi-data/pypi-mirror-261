{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerSimilarity\n",
      "File \u001b[0;32m~/vijil/autoredteam/autoredteam/metrics/metrics.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt_template\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EVAL_STEP1_TEMPLATE, EVAL_STEP2_TEMPLATE\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseScorer\u001b[39;00m(ABC):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, yaml_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from metrics import AnswerSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Score (strings): 0\n",
      "Score (embeddings): 0.749\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the class\n",
    "similarity_scorer = AnswerSimilarity(threshold=0.5)\n",
    "\n",
    "# Use the class with strings and return binary\n",
    "ground_truth_string = \"This is a test.\"\n",
    "answer_string = \"This is a test.\"\n",
    "binary_score_string = similarity_scorer.score(ground_truth_string, answer_string, return_binary=True)\n",
    "print(f\"Binary Score (strings): {binary_score_string}\")\n",
    "\n",
    "# Use the class with embeddings and return score\n",
    "ground_truth_embedding = np.random.rand(768)\n",
    "answer_embedding = np.random.rand(768)\n",
    "score_embedding = similarity_scorer.score(ground_truth_embedding, answer_embedding, return_binary=False)\n",
    "print(f\"Score (embeddings): {score_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = [\"faithfulness\", \"answer_correctness\"]\n",
    "question = ['When was the first super bowl?', 'Who won the most super bowls?']\n",
    "answer = ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots']\n",
    "context = [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'],\n",
    "           ['The Green Bay Packers...Green Bay, Wisconsin.', 'The Packers compete...Football Conference']]\n",
    "ground_truth = ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness, answer_relevancy, answer_similarity, context_entity_recall, context_precision, context_recall, context_relevancy\n",
    "\n",
    "class MetricsEvaluator:\n",
    "    def __init__(self):\n",
    "        self.metrics_dict = {\n",
    "            'faithfulness': faithfulness, \n",
    "            'answer_correctness': answer_correctness,\n",
    "            'answer_relevancy': answer_relevancy,\n",
    "            'answer_similarity': answer_similarity,\n",
    "            'context_entity_recall': context_entity_recall,\n",
    "            'context_precision': context_precision,\n",
    "            'context_recall': context_recall,\n",
    "            'context_relevancy': context_relevancy,                 \n",
    "        }\n",
    "\n",
    "    def score(self, metrics, question, answer, contexts, ground_truth):\n",
    "        # Create a list of metrics modules based on the input list of strings\n",
    "        metrics_modules = [self.metrics_dict[metric] for metric in metrics if metric in self.metrics_dict]\n",
    "\n",
    "        data_samples = {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'contexts': contexts,\n",
    "            'ground_truth': ground_truth\n",
    "        }\n",
    "\n",
    "        dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "        # Use the list of metrics modules in the evaluate function\n",
    "        score = evaluate(dataset, metrics=metrics_modules)\n",
    "        return score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 16/16 [00:02<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         question  \\\n",
      "0  When was the first super bowl?   \n",
      "1   Who won the most super bowls?   \n",
      "\n",
      "                                              answer  \\\n",
      "0       The first superbowl was held on Jan 15, 1967   \n",
      "1  The most super bowls have been won by The New ...   \n",
      "\n",
      "                                            contexts  \\\n",
      "0  [The First AFL–NFL World Championship Game was...   \n",
      "1  [The Green Bay Packers...Green Bay, Wisconsin....   \n",
      "\n",
      "                                        ground_truth  faithfulness  \\\n",
      "0   The first superbowl was held on January 15, 1967           1.0   \n",
      "1  The New England Patriots have won the Super Bo...           NaN   \n",
      "\n",
      "   answer_correctness  answer_relevancy  answer_similarity  \\\n",
      "0            0.749093          0.980723           0.996372   \n",
      "1            0.731086          0.943010           0.924343   \n",
      "\n",
      "   context_entity_recall  context_precision  context_recall  context_relevancy  \n",
      "0                    0.5                1.0             1.0                1.0  \n",
      "1                    0.0                0.0             0.0                0.5  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluator = MetricsEvaluator()\n",
    "metrics = list(evaluator.metrics_dict.keys())\n",
    "question = ['When was the first super bowl?', 'Who won the most super bowls?']\n",
    "answer = ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots']\n",
    "contexts = [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'], \n",
    "['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']]\n",
    "ground_truth = ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "\n",
    "scores = evaluator.score(metrics, question, answer, contexts, ground_truth)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
