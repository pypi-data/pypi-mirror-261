from mlx_lm.utils import load, generate_step
import mlx.core as mx
import re

temperature = 0
context_length = 1000
stop_words = ["<|im_start|>", "<|im_end|>", "<s>", "</s>"]

def generate_steps(the_prompt, the_model, tokenizer):
    tokens = []
    skip = 0

    for (token, prob), n in zip(generate_step(mx.array(tokenizer.encode(the_prompt)), the_model, temperature),
                                range(context_length)):

        if token == tokenizer.eos_token_id:
            break

        tokens.append(token.item())
        text = tokenizer.decode(tokens)

        trim = None

        for sw in stop_words:
            if text[-len(sw):].lower() == sw:
                return
            else:
                for i, _ in enumerate(sw, start=1):
                    if text[-i:].lower() == sw[:i]:
                        trim = -i

        yield text[skip:trim]
        skip = len(text)

def generate(prompt, model, tokenizer):
    response = ''

    for chunk in generate_steps(prompt, model, tokenizer):
        response = response + chunk
        if True:
            response = re.sub(r"^/\*+/", "", response)
            response = re.sub(r"^:+", "", response)
            response = response.replace('ï¿½', '')

    return response