{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: Simple RAG\n",
        "\n",
        "Part 1 of this series will cover the simple RAG application with a retrieval model. Below you can find a visual from our Graph. The idea of this is to create a baseline with evaluation metrics to compare with the more complex RAG pipelines in the following Parts. \n",
        "\n",
        "\n",
        "## RAG Card:\n",
        "\n",
        "The RAG card defines the components of the pipeline and the models used in the pipeline.\n",
        "\n",
        "| | | \n",
        "|---|---|\n",
        "| Generator LLM | [NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) hosted on HF API |\n",
        "| Retriever | [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) with `384` dimension |\n",
        "| Dataset | [philschmid/easyrag-mini-wikipedia](https://huggingface.co/datasets/philschmid/easyrag-mini-wikipedia) |\n",
        "| Evaluator | [NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) | \n",
        "| Metrics | [Answer Correctness](../src/easyrag/metrics/answer_correctness.py), [Answer Faithfulness](../src/easyrag/metrics/answer_faithfulness.py), [Context Precision](../src/easyrag/metrics/context_precision.py), [Context Recall](../src/easyrag/metrics/context_recall.py)   |\n",
        "\n",
        "## Metrics definition\n",
        "* **Answer Correctness**: Evaluates the `answer` with the `ground truth` and returns 0 INCORRECT or 1 CORRECT.\n",
        "* **Answer Faithfulness**: Evaluates if `answer` is \"faithfull\" based on the provided `context` and returns a 0 UNFAITHFUL or 1 FAITHFUL, e.g. I cannot answer since no information is given in the context.\n",
        "* **Context Precision**: Evaluates how many of the retrieved documents are relevant to answer the question, uses `question`, `context` and `ground truth` answer.\n",
        "* **Context Recall**: Evaluates how many sentences in the `answer` can be attributed to retrieved documents, uses `context` and `answer`. \n",
        "\n",
        "\n",
        "## Simple RAG pipeline\n",
        "\n",
        "Below you can find a visual graph of the simple RAG pipeline. \n",
        "\n",
        "<img src=\"../assets/simple-rag.png\" width=\"650\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installing dependencies\n",
        "\n",
        "The first step is to install the necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pip install git+https://github.com/philschmid/easyrag.git faiss-cpu datasets sentence_transformers openai --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Indexing the documents\n",
        "\n",
        "Before we can build and evaluate our RAG pipeline we need to index our dataset and create the embeddings for the retriever. We are going to use the [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) model, [faiss](https://github.com/facebookresearch/faiss) and [philschmid/easyrag-mini-wikipedia](https://huggingface.co/datasets/philschmid/easyrag-mini-wikipedia) dataset.\n",
        "\n",
        "The [philschmid/easyrag-mini-wikipedia](https://huggingface.co/datasets/philschmid/easyrag-mini-wikipedia) is a simple dataset to evaluate RAG pipelines. It consists out of ~900 question and ground truth answers from Wikipedia articles. In addition to the questions it has a second config with ~3,200 documents for retrieval. For evaluation we will use the `mini_100` split from the `questions` config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_documents = load_dataset(\"philschmid/easyrag-mini-wikipedia\",\"documents\",split=\"passages\")\n",
        "\n",
        "print(raw_documents[0][\"document\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to use the [HuggingFaceEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/sentence_transformers) class to locally embedd the documents and store them in a local index. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import BertTokenizerFast\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
        "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
        "    tokenizer, chunk_size=256, chunk_overlap=64,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now start indexing the documents into our Faiss index. We are keeping things simple an chunk into `256` token chunks if the document is longer than this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "chunked_documents = text_splitter.create_documents(raw_documents[\"document\"])\n",
        "print(f\"Chunked documents: {len(chunked_documents)}\")\n",
        "db = FAISS.from_documents(chunked_documents, embeddings)\n",
        "retriever = db.as_retriever()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets test our retriever with a simple question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What is the capital of Germany?\"\n",
        "docs = retriever.invoke(query)\n",
        "print(docs[0].page_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now going to save the index to disk so we can use it later in our langgraph pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db.save_local(\"faiss_index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Building the RAG pipeline with LangGraph\n",
        "\n",
        "LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain. The main use is for adding cycles and controflows to your LLM application. We are not going to use the full capabilities of LangGraph in this part, but we are going to use it in the following parts.\n",
        "\n",
        "Our Nodes in the graph are:\n",
        "* **Retriever**: This node is responsible for retrieving the documents from the index.\n",
        "* **Generator**: This node is responsible for generating the answer based on the retrieved documents.\n",
        "\n",
        "The first step is to define our Graph, with the state paramters. We are going only to have \"question\" and \"documents\" as state parameters. Each node will simply modify the state. Nodes will be connected by edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        query: The query that was used to retrieve the documents.\n",
        "        documents: The documents that were retrieved.\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    context: Dict[str, str]\n",
        "    answer: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we can define our Nodes, which are simple python functions, which take the state as input and return the modified state as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import cast\n",
        "\n",
        "\n",
        "def retrieve(state:GraphState):\n",
        "    # destruct state\n",
        "    question = state.get(\"question\", None)\n",
        "    # retriever = state.get(\"retriever\", None)\n",
        "        \n",
        "    # retrieve top 5 documents\n",
        "    documents = retriever.similarity_search_with_score(question, k=5)\n",
        "    \n",
        "    # reverse docs to get the highest score first and remove the score\n",
        "    documents = [doc[0].page_content for doc in documents]\n",
        "    documents.reverse()\n",
        "    \n",
        "    # add documents to state\n",
        "    return {\"context\": documents}\n",
        "\n",
        "def generate(state):\n",
        "    question = state.get(\"question\", None)\n",
        "    context = state.get(\"context\", None)\n",
        "    # generate answer\n",
        "    answer = chain.invoke({\"question\": question, \"context\": context}) \n",
        "    return {\n",
        "        \"answer\": answer\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we have defined our Nodes, we can build our Graph by creating a `StateGraph` object and adding the nodes to it and then connecting them with edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "\n",
        "# Build graph\n",
        "workflow.add_edge(\"retrieve\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we can run the graph we need to define our retrieval model and the generator model. We are going to use the [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) model for the retriever and the [NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) model for the generator. \n",
        "First we load again the `HuggingFaceEmbeddings` class and our `FaissIndex` class to load the index from disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores.faiss import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "retriever = FAISS.load_local(\"faiss_index\", embeddings,allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the LLM we use the `ChatOpenAI` class with the Hugging Face Inference API. We also create our `chain` which is used as part of the generate node. We create a simple prompt and conctate it as chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/ragu/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "import huggingface_hub\n",
        "\n",
        "from langchain_community.chat_models.openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\n",
        "    openai_api_key=huggingface_hub.get_token(),\n",
        "    openai_api_base=\"https://api-inference.huggingface.co/v1/\",\n",
        "    max_tokens=1024,\n",
        "    temperature=0.1,\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an assistant for question-answering tasks. Use the following pieces of retrieved Context to answer the Question. If the context doesn't provide any helpful information to answer the questiom say that you cannot answer.\"),\n",
        "        (\"human\", \"Question: {question}\\nContext: {context}\"),\n",
        "    ]\n",
        ")\n",
        "chain = prompt | llm | StrOutputParser()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nice. Now lets test our graph with a two question. One with a correct answer and one with an incorrect answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# correct = app.invoke({\"question\": \"Which county was Lincoln born in?\"})\n",
        "# print(correct[\"answer\"])\n",
        "# false = app.invoke({\"question\": \"Which county was Philipp born in?\"})\n",
        "# print(false[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fantastic. The graph is working as expected. We can now move on to the evaluation of the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generating Answers\n",
        "\n",
        "Before we can evaluate the pipeline we need to generate the answers for the questions. We are going to iterate over the questions and generate the answers using our graph. We will use the `mini_100` split from the `questions` config, which includes 100 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44b8b6c0cb7e41d2a485e33ac8cef5af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 48.1k/48.1k [00:00<00:00, 98.2kB/s]\n",
            "Downloading data: 100%|██████████| 8.42k/8.42k [00:00<00:00, 25.5kB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7324b8dbab1e499c8df47ddfb70d2bd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating full split:   0%|          | 0/918 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2603e1fef8ee4c64a6ed1261234a8569",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating mini_100 split:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "test_data = load_dataset(\"philschmid/easyrag-mini-wikipedia\",\"questions\",split=\"mini_100\").select(range(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To accelerate the generation process we are going to use the `AsyncRunner` from `easyrag`, which allows us to parallelize the generation process. We are going to use a max concurrency of 8. It leverages the `async` methods from `langgraph`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "100%|██████████| 100/100 [00:06<00:00, 15.12it/s]\n"
          ]
        }
      ],
      "source": [
        "from easyrag.runner import AsyncRunner\n",
        "\n",
        "async def predict(sample):\n",
        "    answer = await app.ainvoke({\"question\": sample[\"question\"]})\n",
        "    return {\"answer\": answer[\"answer\"], \"context\": answer[\"context\"], \"ground_truth\": sample[\"ground_truth\"], \"question\": sample[\"question\"]}\n",
        "\n",
        "r = AsyncRunner(concurrency_limit=8, callable=predict)\n",
        "results = r.run(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets convert our list back into a `Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e4b90bd3db54e1aa62b89fe10e33a10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "264114"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset \n",
        "\n",
        "results = Dataset.from_list(results)\n",
        "# lets save it just in case\n",
        "results.to_json(\"results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cb4a4974bbf4e7882f81cc81aa832e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "results = load_dataset(\"json\", data_files=\"results.json\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate the RAG pipeline with easyrag \n",
        "\n",
        "For the evaluation we are going to use the [easyrag](https://github.com/philschmid/easyrag) and the [philschmid/easyrag-mini-wikipedia](https://huggingface.co/datasets/philschmid/easyrag-mini-wikipedia) dataset using the [NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) model. First we initalize our metrics from easyrag. Each metric will make one or more requests to the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from easyrag.metrics import (\n",
        "    ContextRecall,\n",
        "    ContextPrecision,\n",
        "    AnswerCorrectness,\n",
        "    AnswerFaithfulness,\n",
        " )\n",
        "\n",
        "\n",
        "cr = ContextRecall(llm=llm, verbose=False) # verbose=True to see the prompts\n",
        "cp = ContextPrecision(llm=llm, verbose=False) # verbose=True to see the prompts\n",
        "ac = AnswerCorrectness(llm=llm, verbose=False) # verbose=True to see the prompts\n",
        "af = AnswerFaithfulness(llm=llm, verbose=False) # verbose=True to see the prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets select a random sample from our results and evaluate it with our metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Did the U.S. join the League of Nations?\n",
            "Ground Truth: no\n",
            "Answer: The U.S. did not join the League of Nations. As mentioned in the context, \"Wilson's own Congress did not accept the League and only four of the original Fourteen Points were implemented fully in Europe.\" Additionally, it states that \"Coolidge saw the landslide Republican victory of 1920 as a rejection of the Wilsonian idea that the United States should join the League of Nations.\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api-inference.huggingface.co/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context Recall: 0.0\n",
            "Context Precision: 0.4\n",
            "Answer Correctness: 1.0\n",
            "Answer Faithfulness: 1.0\n"
          ]
        }
      ],
      "source": [
        "from random import randrange\n",
        "\n",
        "sample = results[randrange(len(results))]\n",
        "print(f\"Question: {sample['question']}\")\n",
        "print(f\"Ground Truth: {sample['ground_truth']}\")\n",
        "print(f\"Answer: {sample['answer']}\")\n",
        "\n",
        "cr_score = cr.compute(context=sample[\"context\"],ground_truth=sample[\"ground_truth\"])\n",
        "cp_score = cp.compute(context=sample[\"context\"],ground_truth=sample[\"ground_truth\"],question=sample[\"question\"])\n",
        "ac_score = ac.compute(answer=sample[\"answer\"],ground_truth=sample[\"ground_truth\"],question=sample[\"question\"])\n",
        "af_score = af.compute(answer=sample[\"answer\"],context=sample[\"context\"])\n",
        "\n",
        "print(f\"Context Recall: {cr_score}\")\n",
        "print(f\"Context Precision: {cp_score}\")\n",
        "print(f\"Answer Correctness: {ac_score}\")\n",
        "print(f\"Answer Faithfulness: {af_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nice. The metrics are working as expected. We can now run the evaluation on the whole dataset. Therefore we will use again our `AsyncRunner` from `easyrag`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1/100 [00:01<02:17,  1.39s/it]ERROR:easyrag.metrics.answer_correctness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The answer provided does not directly answer the question, as it does not explicitly state the most popular rock group in Finland. Instead, it offers a list of popular Finnish rock bands and mentions that CMX is arguably one of Finland's most domestically popular rock groups. While this information is relevant, it does not definitively answer the question. The answer should have clearly stated the most popular rock group in Finland based on the provided context or acknowledged the lack of definitive information.\", \"score\": \"0\"}}\n",
            "question: \"What is the capital of Australia?\"\n",
            "ground_truth: \"The capital of Australia is Canberra.\"\n",
            "answer: \"Canberra is the capital city of Australia.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the capital of Australia, and the answer correctly states that it is Canberra, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the largest planet in our solar system?\"\n",
            "ground_truth: \"The largest planet in our solar system is Jupiter.\"\n",
            "answer: \"Jupiter is the largest planet in our solar system.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the largest planet in our solar system, and the answer correctly states that it is Jupiter, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the chemical formula for water?\"\n",
            "ground_truth: \"The chemical formula for water is H2O.\"\n",
            "answer: \"Water has the chemical formula H2O.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the chemical formula for water, and the answer correctly states that it is H2O, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the tallest mountain in the world?\"\n",
            "ground_truth: \"The tallest mountain in the world is Mount Everest, with an elevation of 8,848 meters (29,029 feet) above sea level.\"\n",
            "answer: \"Mount Everest is the tallest mountain in the world, with a height of 8,848 meters (29,029 feet) above sea level.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the tallest mountain in the world, and the answer correctly states that it is Mount Everest, with a height of 8,848 meters (29,029 feet) above sea level, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the currency of Japan?\"\n",
            "ground_truth: \"The currency of Japan is the Japanese yen (JPY).\"\n",
            "answer: \"The currency of Japan is the Japanese yen (JPY).\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the currency of Japan, and the answer correctly states that it is the Japanese yen (JPY), which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the largest organ in the human body?\"\n",
            "ground_truth: \"The largest organ in the human body is the skin.\"\n",
            "answer: \"The skin is the largest organ in the human body.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the largest organ in the human body, and the answer correctly states that it is the skin, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the chemical symbol for gold?\"\n",
            "ground_truth: \"The chemical symbol for gold is Au.\"\n",
            "answer: \"The chemical symbol for gold is Au.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the chemical symbol for gold, and the answer correctly states that it is Au, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the largest country in the world by land area?\"\n",
            "ground_truth: \"The largest country in the world by land area is Russia, with a total area of 17,098,242 square kilometers (6,601,668 square miles).\"\n",
            "answer: \"Russia is the largest country in the world by land area, with a total area of 17,098, returning None\n",
            " 12%|█▏        | 12/100 [00:01<00:08, 10.78it/s]ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"no\", \"reason\": \"The given answer is not a sentence and cannot be classified as attributed or not.\", \"Attributed\": \"0\"}]}\n",
            "\n",
            "Note: The answer provided is not a sentence, so it cannot be classified as attributed or not. However, if the answer was a sentence, the classification would be based on whether the sentence can be found or inferred from the given context., returning None\n",
            "ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"The \"Colossus of Independence\".\", \"reason\": \"The context mentions that Jefferson referred to Adams as 'The Colossus of that Congress' and 'the great pillar of support to the Declaration of Independence, and its ablest advocate and champion on the floor of the House.'\", \"Attributed\": \"1\"}]}, returning None\n",
            "ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"yes\", \"reason\": \"The given context does not contain a specific statement to classify.\", \"Attributed\": \"0\"}]}\n",
            "\n",
            "Note: The provided answer \"yes\" does not have a corresponding sentence in the context to classify. Therefore, the classification is marked as '0'., returning None\n",
            "ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"yes\", \"reason\": \"The given context does not contain any sentences to classify.\", \"Attributed\": \"0\"}]}\n",
            "\n",
            "Note: The answer \"yes\" is not a complete sentence, so it cannot be classified as being attributed to the given context., returning None\n",
            "ERROR:easyrag.metrics.context_precision:Response from LLM is not valid JSON: {\"verification\": {\"reason\": \"The provided context discusses the longevity of turtles and their organs, but it does not directly address the question about turtles taking many years to reach breeding age. However, the context does imply that turtles have a long lifespan, which could indirectly suggest that they may take many years to reach breeding age.\", \"score\": \"0.5\"}}\n",
            "question: \"What is the capital of Australia?\"\n",
            "context: \"Australia, officially the Commonwealth of Australia, is a sovereign country comprising the mainland of the Australian continent, the island of Tasmania, and numerous smaller islands. It is the largest country in Oceania and the world's sixth-largest country by total area. The neighboring countries are Papua New Guinea, Indonesia, and East Timor to the north; the Solomon Islands and Vanuatu to the north-east; and New Zealand to the south-east. Australia's capital is Canberra, and its largest city is Sydney.\"\n",
            "answer: \"Canberra\"\n",
            "{\"verification\": {\"reason\": \"The provided context clearly states that Australia's capital is Canberra, which directly answers the question.\", \"score\": \"1\"}}, returning None\n",
            "ERROR:easyrag.metrics.answer_correctness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks if there are a large number of Jews living in Egypt today, and the answer correctly states that there are not, as the Jewish community in Egypt numbers less than 500. This response aligns with the provided ground truth, which simply states 'no', indicating that the answer is accurate and relevant to the question.\", \"score\": \"score\": \"1\"}}\n",
            ", returning None\n",
            "ERROR:easyrag.metrics.answer_correctness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the counter to Cleveland's innocent image in 1884, and the answer correctly states that his opponents accused him of fathering an illegitimate child. The answer also includes the additional information about the phrase \"Ma, Ma, where's my Pa?\" being used as a derisive slogan against him, which further supports the accuracy of the response.\", \"score\": \"1\"}}\n",
            ", returning None\n",
            " 31%|███       | 31/100 [00:02<00:03, 18.17it/s]ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"Yes.\", \"reason\": \"The given context does not contain any sentences to classify.\", \"Attributed\": \"0\"}]}\n",
            "\n",
            "Note: The provided answer \"Yes\" is not a complete sentence, so it cannot be classified as being attributed to the given context., returning None\n",
            "ERROR:easyrag.metrics.context_precision:Response from LLM is not valid JSON: {\"verification\": {\"reason\": \"The provided context does not mention Fredericksburg, so it cannot be confirmed that the person practiced law in Fredericksburg based on this information.\", \"score\": \"0\"}}\n",
            "question: \"What is the capital of France?\"\n",
            "context: \"France, officially the French Republic, is a country primarily located in Western Europe, consisting of metropolitan France and several overseas regions and territories. The metropolitan area of France extends from the Mediterranean Sea to the English Channel and the North Sea, and from the Rhine to the Atlantic Ocean. Paris is the capital and most populous city of France, with an estimated population of 2.4 million.\"\n",
            "{\"verification\": {\"reason\": \"The provided context clearly states that Paris is the capital of France.\", \"score\": \"1\"}}\n",
            ", returning None\n",
            "ERROR:easyrag.metrics.context_precision:Response from LLM is not valid JSON: {\"verification\": {\"reason\": \"The provided context discusses the natural predators of kangaroos, such as dingos, foxes, and feral cats. This information contradicts the answer 'no', which implies that kangaroos have few or no natural predators.\", \"score\": \"0\"}}\n",
            "Question: \"Do kangaroos have many natural predators?\"\n",
            "Context: \"Along with dingos and other canids, introduced species like foxes and feral cats also pose a threat to kangaroo populations. Kangaroos and wallabies are adept swimmers, and often flee into waterways if presented with the option. If pursued into the water, a large kangaroo may use its forepaws to hold the predator underwater so as to drown it. Another defensive tactic described by witnesses is catching the attacking dog with the forepaws and disembowelling it with the hind legs.\"\n",
            "Answer: \"yes\"\n",
            "Verification: {\"reason\": \"The provided context discusses the natural predators of kangaroos, such as dingos, foxes, and feral cats. This information directly relates to the question about whether kangaroos have many natural predators.\", \"score\": \"1\"}}, returning None\n",
            " 39%|███▉      | 39/100 [00:03<00:04, 13.20it/s]ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"yes\", \"reason\": \"The given context does not contain any sentences to classify.\", \"Attributed\": \"0\"}]}\n",
            "\n",
            "Note: The answer provided is a single word \"yes\" which cannot be classified as a sentence from the context. Therefore, the classification is marked as '0'., returning None\n",
            " 48%|████▊     | 48/100 [00:03<00:02, 18.60it/s]ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"yes\", \"reason\": \"The context provides information about Avogadro's Law, Avogadro's number, and its significance in chemistry, but the answer 'yes' does not correspond to any specific sentence in the context.\", \"Attributed\": \"0\"}]}\n",
            "\n",
            "Note: The answer \"yes\" is not a complete sentence and cannot be attributed to the context., returning None\n",
            "ERROR:easyrag.metrics.context_precision:Response from LLM is not valid JSON: {\"verification\": {\"reason\": \"The provided context explains Avogadro's number and its significance in chemistry, but it does not directly address the question of whether it is commonly used to compute the results of chemical reactions. However, the context does imply that Avogadro's number is important in chemistry, which suggests that it is likely used in chemical calculations.\", \"score\": \"0.5\"}}\n",
            "question: \"What is the capital of Australia?\"\n",
            "context: \"Australia, officially the Commonwealth of Australia, is a sovereign country comprising the mainland of the Australian continent, the island of Tasmania, and numerous smaller islands. It is the largest country in Oceania and the world's sixth-largest country by total area. The neighboring countries are Papua New Guinea, Indonesia, and East Timor to the north; the Solomon Islands and Vanuatu to the north-east; and New Zealand to the south-east. Australia's capital is Canberra, and its largest city is Sydney.\"\n",
            "answer: \"Canberra\"\n",
            "{\"verification\": {\"reason\": \"The provided context clearly states that Australia's capital is Canberra, which directly answers the question.\", \"score\": \"1\"}}\n",
            "question: \"What is the largest planet in our solar system?\"\n",
            "context: \"The Solar System is the gravitationally bound system of the Sun and the objects that orbit it, either directly or indirectly. Of the objects that orbit the Sun directly, the largest is the gas giant Jupiter, followed by Saturn, Uranus, and Neptune. The Solar System also contains smaller objects, such as the dwarf planets Pluto, Haumea, Makemake, and Eris, as well as many other smaller bodies, including asteroids, meteoroids, and comets.\"\n",
            "answer: \"Jupiter\"\n",
            "{\"verification\": {\"reason\": \"The provided context clearly states that the largest planet in our solar system is Jupiter, which directly answers the question.\", \"score\": \"1\"}}\n",
            "question: \"What is the primary language spoken in Brazil?\"\n",
            "context: \"Brazil, officially the Federative Republic of Brazil, is the largest country in both South America and Latin America. Portuguese is the official language of Brazil and is spoken by the majority of the population. The country is known for its rich culture, diverse landscapes, and vibrant cities, including Rio de Janeiro and São Paulo.\"\n",
            "answer: \"Portuguese\"\n",
            "{\"verification\": {\"reason\": \"The provided context clearly states that Portuguese is the official language of Brazil, which directly answers the question.\", \"score\": \"1\"}}, returning None\n",
            "ERROR:easyrag.metrics.answer_correctness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks if Grover Cleveland won the 1884 election, and the answer correctly states that he did, as mentioned in the provided ground truth. The answer specifically addresses the condition \"1884 election,\" which is crucial because it directly answers the question.\", \"score\": \"1\"}}\n",
            ", returning None\n",
            " 54%|█████▍    | 54/100 [00:04<00:03, 12.91it/s]ERROR:easyrag.metrics.answer_correctness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The answer provided is correct based on the context given. The question asks for the boiling point of water, and the answer correctly states that it is 100 degrees Celsius at sea level, which is in agreement with the context provided. The answer specifically addresses the condition \"at sea level,\" which is crucial because, as noted in the context, the boiling point of water can change with altitude.\", \"score\": \"1\"}}, returning None\n",
            " 64%|██████▍   | 64/100 [00:04<00:01, 18.64it/s]ERROR:easyrag.metrics.answer_correctness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the individual who lost control of his party to the agrarians and silverites in 1896, and the answer correctly states that it was Grover Cleveland. The answer specifically addresses the condition \"in 1896,\" which is crucial because it identifies the specific time period in which the event occurred.\", \"score\": \"1\"}}, returning None\n",
            " 70%|███████   | 70/100 [00:05<00:02, 13.28it/s]ERROR:easyrag.metrics.answer_faithfulness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The context provides a direct quote from Lincoln's Gettysburg Address, which states, 'America was born not in 1789 but in 1776, \"conceived in Liberty, and dedicated to the proposition that all men are created equal.\"', directly supporting the answer.\", \"score\": \"1\"}}\n",
            ", returning None\n",
            "ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"Howling helps pack members keep in touch, allowing them to communicate effectively in thickly forested areas or over great distances. Howling also helps to call pack members to a specific location. Howling can also serve as a declaration of territory, as shown in a dominant wolf's tendency to respond to a human imitation of a \"rival\" wolf in an area the wolf considers its own. \", \"reason\": \"The given sentence is a direct quote from the context.\", \"Attributed\": \"1\"}]}, returning None\n",
            " 80%|████████  | 80/100 [00:05<00:01, 18.78it/s]ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"yes\", \"reason\": \"The given context does not contain any sentences to classify.\", \"Attributed\": \"0\"}]}\n",
            "\n",
            "Note: The provided answer \"yes\" is not a complete sentence, so it cannot be classified as being attributed to the given context., returning None\n",
            "ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"yes\", \"reason\": \"The given answer is not a sentence and cannot be classified as attributed or not attributed to the context.\", \"Attributed\": \"0\"}]}\n",
            "\n",
            "Note: The given answer \"yes\" is not a valid sentence to classify in the context of the provided information., returning None\n",
            "ERROR:easyrag.metrics.context_precision:Response from LLM is not valid JSON: {\"verification\": {\"reason\": \"The provided context does not contain any information about the remains nicknamed 'John of Anina' or their age. Therefore, the context was not useful in arriving at the given answer.\", \"score\": \"0\"}}\n",
            "question: \"What is the capital of Australia?\"\n",
            "context: \"Australia, officially the Commonwealth of Australia, is a sovereign country comprising the mainland of the Australian continent, the island of Tasmania, and numerous smaller islands. It is the largest country in Oceania and the world's sixth-largest country by total area. The neighboring countries are Papua New Guinea, Indonesia, and East Timor to the north; the Solomon Islands and Vanuatu to the north-east; and New Zealand to the south-east. Australia's capital is Canberra, and its largest city is Sydney.\"\n",
            "answer: \"Canberra\"\n",
            "{\"verification\": {\"reason\": \"The provided context clearly states that Australia's capital is Canberra, which directly answers the question. Therefore, the context was useful in arriving at the given answer.\", \"score\": \"1\"}}\n",
            "question: \"What is the largest planet in our solar system?\"\n",
            "context: \"The Solar System is the gravitationally bound system of the Sun and the objects that orbit it, either directly or indirectly. Of the objects that orbit the Sun directly, the largest is the gas giant Jupiter, which is the fifth planet from the Sun. Jupiter is the largest planet in our solar system, with a diameter of about 86,881 miles (139,822 kilometers).\"\n",
            "answer: \"Jupiter\"\n",
            "{\"verification\": {\"reason\": \"The provided context directly states that Jupiter is the largest planet in our solar system, which answers the question. Therefore, the context was useful in arriving at the given answer.\", \"score\": \"1\"}}\n",
            "question: \"What is the chemical symbol for gold?\"\n",
            "context: \"The chemical elements are a set of elementary substances, consisting of one type of atom, that generate all the chemical compounds and are the fundamental building blocks of all matter. Each chemical element is distinguished by its atomic number, which is the number of protons in its nucleus. The chemical elements are organized on the periodic table, which groups elements with similar properties together. The chemical symbol for gold is Au, which comes from the Latin word for gold, 'aurum'.\"\n",
            "answer: \"Au\"\n",
            "{\"verification\": {\"reason\": \"The provided context directly states that the chemical symbol for gold is Au, which answers the question. Therefore, the context was useful in arriving at the given answer.\", \"score\": \"1\"}}\n",
            "question: \"What is the largest organ in the human body?\"\n",
            "context: \"The human body is a complex system made up of various organs and tissues that work together to perform essential functions. The largest organ in the human body is the skin, which serves as a protective barrier against pathogens and helps regulate body temperature. The skin is composed of multiple layers, including the epidermis, dermis, and hypodermis, and covers the entire surface of the body.\"\n",
            "answer: \"Skin\"\n",
            "{\"verification\": {\"reason\": \"The provided context directly states that the largest organ in the human body is the skin, which answers the question. Therefore, the context was useful in arriving at the given answer.\", \"score\": \"1\"}}\n",
            "question: \"What is the currency of Japan?\"\n",
            "context: \"Japan is an island country in East Asia, located in the northwest Pacific Ocean. It is bordered by the Sea of Japan to the west and the East China Sea to the south. Japan is a highly developed country with a high standard of living and a strong economy. The official currency of Japan is the Japanese yen, which is represented by the symbol ¥ and the ISO code JPY.\"\n",
            "answer: \"Japanese yen\"\n",
            "{\"verification\": {\"reason\": \"The provided context directly states that the official currency of Japan is the Japanese yen, which answers the question. Therefore, the context was useful in arriving at the given answer.\", \"score\": \"1\"}}\n",
            "question: \"What is the capital of France?\"\n",
            "context: \"France, officially the French Republic, is a country located primarily in Western Europe. It is bordered by Belgium, Luxembourg, Germany, Switzerland, Italy, Monaco, Andorra, and Spain. France is a developed country with a high standard of living and a strong economy. The capital of France is Paris, which is also the country's largest city.\"\n",
            "answer: \"Paris\"\n",
            "{\"verification\": {\"reason\": \"The provided context directly states that the capital of France is Paris, which answers the question. Therefore, the, returning None\n",
            "ERROR:easyrag.metrics.context_precision:Response from LLM is not valid JSON: {\"verification\": {\"reason\": \"The provided context does not mention Fillmore or the specific situation described in the question. Therefore, the context was not useful in arriving at the given answer.\", \"score\": \"0\"}}\n",
            "question: \"What is the capital of Australia?\"\n",
            "context: \"Australia, officially the Commonwealth of Australia, is a sovereign country comprising the mainland of the Australian continent, the island of Tasmania, and numerous smaller islands. It is the largest country in Oceania and the world's sixth-largest country by total area. The neighboring countries are Papua New Guinea, Indonesia, and East Timor to the north; the Solomon Islands and Vanuatu to the north-east; and New Zealand to the south-east. Australia's capital is Canberra, and its largest city is Sydney.\"\n",
            "answer: \"Canberra\"\n",
            "{\"verification\": {\"reason\": \"The provided context includes the information that Australia's capital is Canberra, which directly relates to the question and was useful in arriving at the given answer.\", \"score\": \"1\"}}\n",
            "question: \"What is the largest planet in our solar system?\"\n",
            "context: \"The Solar System is the gravitationally bound system of the Sun and the objects that orbit it, either directly or indirectly. Of the objects that orbit the Sun directly, the largest is the gas giant Jupiter, which is the fifth planet from the Sun. Jupiter is the largest planet in our solar system, with a diameter of about 86,881 miles (139,822 kilometers).\"\n",
            "answer: \"Jupiter\"\n",
            "{\"verification\": {\"reason\": \"The provided context directly states that Jupiter is the largest planet in our solar system, which is the information needed to answer the question. Therefore, the context was useful in arriving at the given answer.\", \"score\": \"1\"}}\n",
            "question: \"What is the chemical symbol for gold?\"\n",
            "context: \"The chemical elements are a set of elementary substances, each with its own unique atomic number, which uniquely identifies a specific chemical element. Each element is represented by a one- or two-letter chemical symbol, which is used as a shorthand in various chemical contexts. For example, the chemical symbol for gold is Au, which stands for the Latin word 'aurum'.\"\n",
            "answer: \"Au\"\n",
            "{\"verification\": {\"reason\": \"The provided context includes the information that the chemical symbol for gold is Au, which directly relates to the question and was useful in arriving at the given answer.\", \"score\": \"1\"}}\n",
            "question: \"What is the capital of France?\"\n",
            "context: \"France, officially the French Republic, is a country primarily located in Western Europe, consisting of metropolitan France and several overseas regions and territories. The metropolitan area of France extends from the Mediterranean Sea to the English Channel and the North Sea, and from the Rhine to the Atlantic Ocean. France is one of the most visited countries in the world, receiving around 83 million foreign tourists annually. The capital and most populous city of France is Paris.\"\n",
            "answer: \"Paris\"\n",
            "{\"verification\": {\"reason\": \"The provided context includes the information that the capital of France is Paris, which directly relates to the question and was useful in arriving at the given answer.\", \"score\": \"1\"}}\n",
            "question: \"What is the largest organ in the human body?\"\n",
            "context: \"The human body is composed of various organs, each with its own specific function. The largest organ in the human body is the skin, which serves as a protective barrier against pathogens and environmental factors. The skin also plays a crucial role in regulating body temperature and synthesizing vitamin D.\"\n",
            "answer: \"skin\"\n",
            "{\"verification\": {\"reason\": \"The provided context directly states that the largest organ in the human body is the skin, which directly relates to the question and was useful in arriving at the given answer.\", \"score\": \"1\"}}\n",
            "question: \"What is the currency of Japan?\"\n",
            "context: \"Japan is an island country in East Asia, located in the northwest Pacific Ocean. It is bordered by the Sea of Japan to the west and extends from the Sea of Okhotsk in the north to the East China Sea and the Philippine Sea in the south. Japan is a highly developed country with a high standard of living and is known for its advanced technology and rich cultural heritage. The official currency of Japan is the Japanese yen.\"\n",
            "answer: \"Japanese yen\"\n",
            "{\"verification\": {\"reason\": \"The provided context includes the information that the official currency of Japan is the Japanese yen, which directly relates to the question and was useful in arriving at the given answer.\", \"score\": \"1\"}}, returning None\n",
            "ERROR:easyrag.metrics.answer_correctness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The answer provided does not directly address the question asked. The question asks for a specific entity that has its own political legislature and governor, but the answer discusses provinces in Canada and Uruguay without explicitly mentioning which entity has both a political legislature and a governor. While it can be inferred that provinces in Canada have their own political legislatures headed by a Premier, the answer does not clearly state this and does not mention Uruguay's legislative power in the context of having a governor. The answer is vague and does not directly answer the question, making it incorrect.\", \"score\": \"0\"}}\n",
            "question: \"What is the capital of Australia?\"\n",
            "ground_truth: \"The capital of Australia is Canberra.\"\n",
            "answer: \"Canberra is the capital city of Australia.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the capital of Australia, and the answer correctly states that it is Canberra, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the largest planet in our solar system?\"\n",
            "ground_truth: \"The largest planet in our solar system is Jupiter.\"\n",
            "answer: \"Jupiter is the largest planet in our solar system.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the largest planet in our solar system, and the answer correctly states that it is Jupiter, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the primary function of the human heart?\"\n",
            "ground_truth: \"The primary function of the human heart is to pump blood throughout the body, providing oxygen and nutrients to cells and removing waste products.\"\n",
            "answer: \"The primary function of the human heart is to pump blood throughout the body, providing oxygen and nutrients to cells and removing waste products.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the primary function of the human heart, and the answer correctly states that it is to pump blood throughout the body, providing oxygen and nutrients to cells and removing waste products, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the chemical formula for water?\"\n",
            "ground_truth: \"The chemical formula for water is H2O.\"\n",
            "answer: \"Water has the chemical formula H2O.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the chemical formula for water, and the answer correctly states that it is H2O, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the largest organ in the human body?\"\n",
            "ground_truth: \"The largest organ in the human body is the skin.\"\n",
            "answer: \"The skin is the largest organ in the human body.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the largest organ in the human body, and the answer correctly states that it is the skin, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the currency of Japan?\"\n",
            "ground_truth: \"The currency of Japan is the Japanese yen.\"\n",
            "answer: \"The currency of Japan is the Japanese yen.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the currency of Japan, and the answer correctly states that it is the Japanese yen, which is in agreement with the context provided.\", \"score\": \"1\"}}\n",
            "question: \"What is the highest mountain in the world?\"\n",
            "ground_truth: \"The highest mountain in the world is Mount Everest.\"\n",
            "answer: \"Mount Everest is the highest mountain in the world.\"\n",
            "{\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the highest mountain in the world, and the answer correctly states that it is Mount Everest, which is in agreement with the context provided.\", \"score\": \"1\"}}, returning None\n",
            "ERROR:easyrag.metrics.answer_correctness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks for the remains that are approximately 42,000 years old and have been nicknamed \"John of Anina,\" and the answer correctly identifies them as the oldest modern human (Homo sapiens sapiens) remains in Europe, discovered in the \"Cave With Bones\" near Anina in present-day Romania. The answer accurately addresses the question and provides the requested information.\", \"score\": \"1\"}}, returning None\n",
            "ERROR:easyrag.metrics.answer_correctness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The answer provided is correct based on the context given. The question asks if Fillmore turned down the honor, and the answer correctly states that he did, explaining his reasons for doing so. The answer specifically addresses Fillmore's lack of \"literary nor scientific attainment\" and his inability to understand the Latin text of the diploma, which are in line with the context provided. Therefore, the answer is correct.\", \"score\": \"1\"}}\n",
            ", returning None\n",
            " 85%|████████▌ | 85/100 [00:06<00:01, 12.21it/s]ERROR:easyrag.metrics.context_recall:Response from LLM is not valid JSON: {\"classification\": [{\"sentence_1\": \"yes\", \"reason\": \"The context does not contain a direct question, but the answer 'yes' can be attributed to the statement 'The name Canada comes from a St. Lawrence Iroquoian word meaning \"village\" or \"settlement.\"' as it confirms the origin of the name Canada.\", \"Attributed\": \"1\"}]}\n",
            ", returning None\n",
            " 95%|█████████▌| 95/100 [00:06<00:00, 17.80it/s]ERROR:easyrag.metrics.answer_correctness:Response from LLM is not valid JSON: {\"result\": {\"reason\": \"The answer provided is correct based on the question and the context given. The question asks if Romania shares a border with Ukraine, and the answer correctly states that it does, which is in agreement with the context provided. The answer specifically addresses the condition \"with Ukraine,\" which is crucial because, as noted in the context, Romania shares borders with several other countries as well.\", \"score\": \"1\"}}, returning None\n",
            "100%|██████████| 100/100 [00:07<00:00, 13.22it/s]\n"
          ]
        }
      ],
      "source": [
        "from easyrag.runner import AsyncRunner\n",
        "import logging\n",
        "\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "async def calc_metrics(sample):\n",
        "    cr_score = await cr.acompute(context=sample[\"context\"],ground_truth=sample[\"ground_truth\"])\n",
        "    cp_score = await cp.acompute(context=sample[\"context\"],ground_truth=sample[\"ground_truth\"],question=sample[\"question\"])\n",
        "    ac_score = await ac.acompute(answer=sample[\"answer\"],ground_truth=sample[\"ground_truth\"],question=sample[\"question\"])\n",
        "    af_score = await af.acompute(answer=sample[\"answer\"],context=sample[\"context\"])\n",
        "    return {**sample, **{\"context_recall\": cr_score, \"context_precision\": cp_score, \"answer_correctness\": ac_score, \"answer_faithfulness\": af_score}}\n",
        "\n",
        "r = AsyncRunner(concurrency_limit=16, callable=calc_metrics)\n",
        "\n",
        "metrics = r.run(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc5b60e4e75b4a6da6d66b5ebd1dec20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "273739"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset \n",
        "\n",
        "results_with_metrics = Dataset.from_list(metrics)\n",
        "# lets save it just in case\n",
        "results_with_metrics.to_json(\"results_with_metrics.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets calculate our final metrics and print them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context Recall: 57.50%\n",
            "Context Precision: 31.35%\n",
            "Answer Correctness: 75.00%\n",
            "Answer Faithfulness: 89.00%\n"
          ]
        }
      ],
      "source": [
        "metrics = {\n",
        "    \"context_recall\": [],\n",
        "    \"context_precision\": [],\n",
        "    \"answer_correctness\": [],\n",
        "    \"answer_faithfulness\": [],\n",
        "}\n",
        "\n",
        "# flatten the results\n",
        "for sample in results_with_metrics:\n",
        "    metrics[\"context_recall\"].append(sample[\"context_recall\"])\n",
        "    metrics[\"context_precision\"].append(sample[\"context_precision\"])\n",
        "    metrics[\"answer_correctness\"].append(sample[\"answer_correctness\"])\n",
        "    metrics[\"answer_faithfulness\"].append(sample[\"answer_faithfulness\"])\n",
        "\n",
        "# replace None with 0 for the metrics to count them as false\n",
        "metrics = {k: [0 if v is None else v for v in metrics[k]] for k in metrics}\n",
        "\n",
        "    \n",
        "print(f\"Context Recall: {sum(metrics['context_recall'])/len(metrics['context_recall'])*100:.2f}%\")\n",
        "print(f\"Context Precision: {sum(metrics['context_precision'])/len(metrics['context_precision'])*100:.2f}%\")\n",
        "print(f\"Answer Correctness: {sum(metrics['answer_correctness'])/len(metrics['answer_correctness'])*100:.2f}%\")\n",
        "print(f\"Answer Faithfulness: {sum(metrics['answer_faithfulness'])/len(metrics['answer_faithfulness'])*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
